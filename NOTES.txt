Wed, 08 May 2019 13:31:20 +0200

If we set the dimension of the simulation to one, the distribution of blocks
doesn't work. We would need to add another division, like a mpi_block, to divide
the 1D domain into mpi blocks, then each one into task blocks.

What if we use lblock_t to talk about the lowest subdivision block, expected to
run in a task. The mblock_t, to the mpi block.

Wed, 08 May 2019 21:40:47 +0200

In order to exchange particles from task blocks, I need to determine first:
whether the exchange can be done by shared memory, or whether I need MPI. Also I
need to design the exhange protocol to avoid any deadlock/race condition.

Let's first begin with the simple case: There is only one task per MPI process.

Then each specie block should have a list of particles to send, and another to
receive.

Thu, 13 Jun 2019 17:48:04 +0200

The FFTW took 677,640 us with 32 cores and 2 nodes.

With a size of 2048x2048:

Cores	Nodes	Threads	Time FFTW
16	2	1	140 ms
16	2	48?	670 ms

16	2	16	3829 ms
16	2	1	2658 ms

Tue, 23 Jul 2019 17:10:37 +0200

With the new scheduler, the workers are wasting CPU time in the turn of the
master, and the master is wasting CPU time in the turn of the workers:

$ ps a -o comm,pcpu --sort=-pcpu
COMMAND         %CPU
cpic            3794
mft_worker      33.4
mft_worker      33.1
mft_worker      33.0
mft_worker      32.9
mft_worker      32.9
mft_worker      32.9
mft_worker      32.8
mft_worker      32.8
mft_worker      32.7
mft_worker      32.7
mft_worker      32.7
mft_worker      32.7
mft_worker      32.8
mft_worker      32.8
mft_worker      32.8
mft_worker      32.7
mft_worker      32.7
mft_worker      32.7
mft_worker      32.8
mft_worker      32.7
mft_worker      32.7
mft_worker      32.6
mft_worker      32.6
mft_worker      32.6
mft_worker      32.6
mft_worker      32.5
mft_worker      32.6
mft_worker      32.6
mft_worker      32.5
mft_worker      32.5
mft_worker      32.5
mft_worker      32.4
pmi_proxy        0.3
mpiexec.hydra    0.1

