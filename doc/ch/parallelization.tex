\chapter{Parallelization techniques}

\section{Message Passing Interface}

From the need of standarize communications in a distributed computing
environment, the first draft was proposed in 1992 at the Workshop on Standards
for Message Passing in a Distributed Memory Environment, and has now become one
of the most used communication protocol in HPC. The Message Passing Interface
(MPI) provides a simple to use set of routines to allow processes distributed
among different nodes to comunicate efficiently.

\subsection{Concepts}


\paragraph{Communicator} A communicator refers to a group of processes, in which
each has assigned a unique identifier called the \textit{rank}.

\paragraph{Point-to-point communication} In order for a process to exchange
information with another process, the MPI standard defines what are called
point-to-point communication routines. The most common examples are
\texttt{MPI\_Send} to send data, and \texttt{MPI\_Recv} for the reception.
Both routines need the process rank of the process to stablish the connection.
Additionally a tag is used to label each message, which can be specified in the
reception to filter other messages.

\paragraph{Blocking communication} The standard defines various types of
communication methods for sending and receiving data. The so called blocking
routines are designed such that the call does not return until the communication
has been done. In the \texttt{MPI\_Send} case, the call returns when the sending
data can be safely modified, as has been sent or buffered. In the case of
\texttt{MPI\_Recv} the routine only returns when the data has been received.

\paragraph{Non-blocking communication} Similarly as with the blocking
communication, the routines \texttt{MPI\_Isend} and \texttt{MPI\_Irecv} don't
wait until the message is sent or received to return. They return inmediately,
and the communication status can be checked with \texttt{MPI\_Test} or the
process can wait until the communication request has finished with
\texttt{MPI\_Wait}.

\subsection{Implementations}


\section{OmpSs-2}

OmpSs-2 is the next generation of the OmpSs programming model, composed of a set
of directives and library routines. Mixes from OpenMP the annotation of source
code to parallelize some sections with the StarSs execution model, based on a
thread-pool design pattern.

\subsection{Concepts}

\paragraph{Task} In OmpSs-2 a task is a section of code that can be executed
independently by the runtime schedule. A task may have associated dependencies
which lets the scheduler determine in wich order is allowed to execute the
tasks. The notation used to describe a task is by the utilization of the
\texttt{\#pragma} directive, por example:
%
\begin{lstlisting}
#pragma oss task inout(a[0:N-1]) in(b[0:N-1])
for(i=0; i < N; i++)
	a[i] += b[i];
\end{lstlisting}
%

\paragraph{Parallelization} Unless there is a unmet dependency, all tasks ready
to run are executed in parallel, up to the number of CPU cores available to the
runtime.

\paragraph{Task syncronization} It may be possible that at some point in the
execution all pending task are required to finish in order to continue. The
directive \texttt{taskwait} allows the programmer to specify that the runtime
must wait for completion of all previous created tasks.
